---
title: "Final Practice - Machine Learning Course"
author: "Pedro A. Alonso Baigorri"
date: "21 de marzo de 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = F, eval = T, results='hide', messages=F, warning=F}
# loading the previously calculated models to save execution time
load("~/predict.RData")

#libraries
require("lattice", quietly=TRUE) 
require("ggplot2", quietly=TRUE) 
library(caret)
library(RANN)

```


## 1 - Executive Summary

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

In this project, the goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har

With the provided dataset, the objective is to train different models to be able to classify the ways that the users are doing the exercise based on the data obtained from the different accelerometers.

Once I have the model with more accuracy, I will estimated the In out Sample and Out of Sample Errors in order to test the accuracy of the model outside of the training dataset.

## 2 - Getting the data

First step I will download locally and load the training and testing data from th provided repository:

```{r eval=F}

# Getting the data
{
    trainF <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
    testF <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
    
    if (!file.exists("./data/pml-training.csv"))
    {
        if (!file.exists("./data")){dir.create("./data")}
        download.file(trainF, "./data/pml-training.csv")
    }
    
    if (!file.exists("./data/pml-testing.csv"))
    {
        if (!file.exists("./data")){dir.create("./data")}
        download.file(testF, "./data/pml-testing.csv")
    }
}

# Opening data
train <- read.table(trainF, header = TRUE, sep = ",")
test <- read.table(testF, header = TRUE, sep = ",")


```

## 3 - Data Exploration and selection of variables

Once I have loaded the data I will see the dimensions and variables included in the dataset. I will also look at the variable $classe whch the variable to predict.

```{r eval=F}
# exploring data
dim(train)
dim(test)
```

```{r eval=T}
table(train$classe)
```

As the number of columns (variables) in the dataset is too big, I will try to reduce the number removing the less relevant variables for the prediction.

On this sense, the first 6 variables are relative to user identification, time and other information that doesn't provide any value for the prediction, so they can be removed from the dataset.

```{r eval=F}
#removing time, user name and other variables out of interest
train2 <- train[, -c(1:7)]
test2 <- test[, -c(1:7)]
```

Now I will try to reduce the variables with near zero variance looking at the training and testing dataset.

```{r eval=F}
#analyzing variance in train and test
var <- nearZeroVar(train2, saveMetrics=TRUE)
var <- nearZeroVar(test2, saveMetrics=TRUE)
```

```{r eval=T}
table(var$nzv)
table(var$nzv)
```

As the number of variables with zero variance is lower in the testing dataset I will keep only these variables on both datasets.

```{r eval=F}
# removing variables of near zero variance at test
trainB <- train2[, -nearZeroVar(test2)]
testB <- test2[, -nearZeroVar(test2)]
```

```{r eval=T}
dim(trainB)
dim(testB)
```


## 4 - Training models

Next step is to train different models and get the accuracy of each model. I will train:
* a simple Decision Tree
* a Gradiant Boosting (GBM)
* a C5.0

In order to avoid overfitting I will use cross validation with 10 folds.

```{r eval=F}
# setup of models
set.seed(300)
train_control<- trainControl(method="cv", number=10, savePredictions = TRUE)

#train models decision tree
fitTree <- train(classe ~ .,  data = trainB, method="rpart", trControl=train_control)

#train models C5
fitC5 <- train(classe ~ .,  data = trainB, method="C5.0",  trControl=train_control)

#train models GBM
fitGbm <- train(classe ~ .,  data = trainB, method="gbm", verbose = FALSE, trControl=train_control)

```

Once I have trained the three models followingthe performance of each model is showed.

```{r eval=T}
#printing model results
print(fitTree);plot(fitTree)
print(fitC5); plot(fitC5)
print(fitGbm);plot(fitGbm)
```

According to these results the best model is the C5.0 with a 99'6% of Accuracy.

## 4 - In & Out of Sample Errors

To calculate the In of sample errors I will use the training dataset to the predictor and compare with the real value.

```{r eval=T}
# in of sample errors
pred <- predict(fitC5)
IOS_errors_c5 <- sum(pred != trainB$classe)
IOS_errors_c5_rate <- IOS_errors_c5 / nrow(trainB)
```

According to the results, and with the training dataset the obtained model is a perfect model with 0 errors.

To calculate the out of sample errors as I don't have any validation dataset I will calculate the errors in each of the cross validation folders. The average of these errors will be the estimation for the Out of Sample Errors.

```{r eval=T}
# out of sample errors
pred <- subset(fitC5$pred, trials == 20 & model == "rules" & winnow == FALSE )
OOS_errors_c5 <- tapply(pred$pred != pred$obs, pred$Resample, sum)
n <- table(pred$Resample)
OOS_errors_rate_c5 <- mean(OOS_errors_c5/n)
print(round(OOS_errors_rate_c5, 3))
```

According to this the OoS error is `r round(OOS_errors_rate_c5, 3)`

## 5 - Final Prediction & Conclusions

Now I will apply the selected model to the testing dataset to get the results for the quizz.

```{r eval=T}
prediction <- predict(fitC5, newdata = testB)
print(prediction)
```


As a final conclusion, I have got a very good clasiffier using the C5.0 algorithm with an 99,6% of accuracy and and estimation of OoS of 
`r round(OOS_errors_rate_c5, 3)`
